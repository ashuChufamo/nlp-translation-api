{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSttvWZAu6hM"
      },
      "outputs": [],
      "source": [
        "# CONFIGURE TRAINING SETTINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyPLSiI0u6hN"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXpWAowtu6hN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import nltk\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "from enum import Enum\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "from tokenizers import Tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from abc import ABC, abstractmethod\n",
        "from torch.utils.data import Dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# from torchmetrics.text import BLEUScore, WordErrorRate, CharErrorRate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMUQsZrvUxHf"
      },
      "outputs": [],
      "source": [
        "SRC_LANG = \"en\"\n",
        "TGT_LANG = \"am\"\n",
        "SEQ_LEN = 52\n",
        "# Connect to Google Drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "PROJECT_DIR = \"/content/drive/My Drive/Ashu_NLP\"\n",
        "DATASET_PATH = f\"{PROJECT_DIR}/data/parallel-corpus-en-am-v3.5.json\"\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRHhFhdvu6hO"
      },
      "outputs": [],
      "source": [
        "class TextPreprocessingPipeline(ABC):\n",
        "    def __init__(self, tokenizer: Tokenizer) -> None:\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize the input text into words.\n",
        "        \"\"\"\n",
        "        words = word_tokenize(text)\n",
        "        return words\n",
        "\n",
        "    @abstractmethod\n",
        "    def preprocess(self, text: str, encode=True) -> str:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC71v00vu6hO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import nltk\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "from enum import Enum\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "from tokenizers import Tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from abc import ABC, abstractmethod\n",
        "from torch.utils.data import Dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# from torchmetrics.text import BLEUScore, WordErrorRate, CharErrorRate\n",
        "import pickle\n",
        "\n",
        "# Connect to Google Drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "PROJECT_DIR = \"/content/drive/My Drive/Ashu_NLP\"\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "random.seed(42)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-04\n",
        "MAX_SEQ_LEN = 52\n",
        "MODEL_DIM = 512\n",
        "NUM_LAYERS = 6\n",
        "NUM_HEADS = 32\n",
        "DROPOUT_RATE = 0.1\n",
        "FF_DIM = 2048\n",
        "SOURCE_LANG = \"en\"\n",
        "TARGET_LANG = \"am\"\n",
        "MODEL_DIR = f\"{PROJECT_DIR}/models\"\n",
        "MODEL_PREFIX = \"custom_model_\"\n",
        "PRELOAD_SUFFIX = \"\"\n",
        "TOKENIZER_DIR = f\"{PROJECT_DIR}/tokenizers\"\n",
        "TOKENIZER_PREFIX = \"tokenizer-{0}-v3.5-12k.json\"\n",
        "LOG_DIR = f\"{PROJECT_DIR}/logs/custom_model\"\n",
        "DATA_PATH = f\"{PROJECT_DIR}/data/parallel-corpus-en-am-v3.5.json\"\n",
        "\n",
        "def get_model_path(suffix: str):\n",
        "    return f\"{MODEL_DIR}/{MODEL_PREFIX}{suffix}.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5WXYKXqu6hP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import pickle\n",
        "\n",
        "class AmharicTextPreprocessor(TextPreprocessingPipeline):\n",
        "    def __init__(self, tokenizer: Tokenizer) -> None:\n",
        "        super().__init__(tokenizer)\n",
        "\n",
        "    def preprocess(self, text: str, encode=True) -> str:\n",
        "        # Normalize character level discrepancies\n",
        "        text = self.normalize_char_discrepancies(text)\n",
        "\n",
        "        # Replace common abbreviations\n",
        "        text = self.replace_abbreviations(text)\n",
        "\n",
        "        # Remove punctuation and special characters\n",
        "        text = self.remove_punctuation_and_special_chars(text)\n",
        "\n",
        "        # Remove non-Amharic characters and numbers\n",
        "        text = self.remove_non_amharic_chars_and_numbers(text)\n",
        "\n",
        "        if encode:\n",
        "            return self.tokenizer.encode(text).ids\n",
        "        else:\n",
        "            return text\n",
        "\n",
        "    def replace_abbreviations(self, text: str) -> str:\n",
        "        amharic_abbreviations = {\n",
        "            \"ት/ቤት\": \"ትምህርት ቤት\",\n",
        "            \"ት/ርት\": \"ትምህርት\",\n",
        "            \"ት/ክፍል\": \"ትምህርት ክፍል\",\n",
        "            \"ሃ/አለቃ\": \"ሃምሳ አለቃ\",\n",
        "            \"ሃ/ስላሴ\": \"ሃይለ ስላሴ\",\n",
        "            \"ደ/ዘይት\": \"ደብረ ዘይት\",\n",
        "            \"ደ/ታቦር\": \"ደብረ ታቦር\",\n",
        "            \"መ/ር\": \"መምህር\",\n",
        "            \"መ/ቤት\": \"መስሪያ ቤት\",\n",
        "            \"መ/አለቃ\": \"መቶ አለቃ\",\n",
        "            \"ክ/ከተማ\": \"ክፍለ ከተማ\",\n",
        "            \"ክ/ሀገር\": \"ክፍለ ሀገር\",\n",
        "            \"ወ/ር\": \"\",\n",
        "            \"ወ/ሮ\": \"ወይዘሮ\",\n",
        "            \"ወ/ሪት\": \"ወይዘሪት\",\n",
        "            \"ወ/ስላሴ\": \"ወልደ ስላሴ\",\n",
        "            \"ፍ/ስላሴ\": \"ፍቅረ ስላሴ\",\n",
        "            \"ፍ/ቤት\": \"ፍርድ ቤት\",\n",
        "            \"ጽ/ቤት\": \"ጽህፈት ቤት\",\n",
        "            \"ሲ/ር\": \"\",\n",
        "            \"ፕ/ር\": \"ፕሮፌሰር\",\n",
        "            \"ጠ/ሚንስትር\": \"ጠቅላይ ሚኒስተር\",\n",
        "            \"ጠ/ሚ\": \"ጠቅላይ ሚኒስተር\",\n",
        "            \"ዶ/ር\": \"ዶክተር\",\n",
        "            \"ገ/ገዮርጊስ\": \"ገብረ ገዮርጊስ\",\n",
        "            \"ቤ/ክርስትያን\": \"ቤተ ክርስትያን\",\n",
        "            \"ም/ስራ\": \"\",\n",
        "            \"ም/ቤት\": \"ምክር ቤተ\",\n",
        "            \"ተ/ሃይማኖት\": \"ተክለ ሃይማኖት\",\n",
        "            \"ሚ/ር\": \"ሚኒስትር\",\n",
        "            \"ኮ/ል\": \"ኮሎኔል\",\n",
        "            \"ሜ/ጀነራል\": \"ሜጀር ጀነራል\",\n",
        "            \"ብ/ጀነራል\": \"ብርጋደር ጀነራል\",\n",
        "            \"ሌ/ኮለኔል\": \"ሌተናንት ኮለኔል\",\n",
        "            \"ሊ/መንበር\": \"ሊቀ መንበር\",\n",
        "            \"አ/አ\": \"ኣዲስ ኣበባ\",\n",
        "            \"አ.አ\": \"ኣዲስ ኣበባ\",\n",
        "            \"ር/መምህር\": \"ርዕሰ መምህር\",\n",
        "            \"ፕ/ት\": \"\",\n",
        "            \"ዓም\": \"ዓመተ ምህረት\",\n",
        "            \"ዓ.ዓ\": \"ዓመተ ዓለም\",\n",
        "        }\n",
        "        for key in amharic_abbreviations:\n",
        "            regex = rf'\\b{re.escape(key)}\\b'\n",
        "            text = re.sub(regex, amharic_abbreviations[key], text)\n",
        "        text = re.sub(r'[.\\?\\\"\\',/#!$%^&*;:፤።{}=\\-_`~()፩፪፫፬፭፮፯፰፱፲፳፴፵፶፷፸፹፺፻0-9]+', ' ', text)\n",
        "        text = re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def normalize_char_discrepancies(self, text: str) -> str:\n",
        "        rep1 = re.sub('[ሃኅኃሐሓኻ]', 'ሀ', text)\n",
        "        rep2 = re.sub('[ሑኁዅ]', 'ሁ', rep1)\n",
        "        rep3 = re.sub('[ኂሒኺ]', 'ሂ', rep2)\n",
        "        rep4 = re.sub('[ኌሔዄ]', 'ሄ', rep3)\n",
        "        rep5 = re.sub('[ሕኅ]', 'ህ', rep4)\n",
        "        rep6 = re.sub('[ኆሖኾ]', 'ሆ', rep5)\n",
        "        rep7 = re.sub('[ሠ]', 'ሰ', rep6)\n",
        "        rep8 = re.sub('[ሡ]', 'ሱ', rep7)\n",
        "        rep9 = re.sub('[ሢ]', 'ሲ', rep8)\n",
        "        rep10 = re.sub('[ሣ]', 'ሳ', rep9)\n",
        "        rep11 = re.sub('[ሤ]', 'ሴ', rep10)\n",
        "        rep12 = re.sub('[ሥ]', 'ስ', rep11)\n",
        "        rep13 = re.sub('[ሦ]', 'ሶ', rep12)\n",
        "        rep14 = re.sub('[ዓኣዐ]', 'አ', rep13)\n",
        "        rep15 = re.sub('[ዑ]', 'ኡ', rep14)\n",
        "        rep16 = re.sub('[ዒ]', 'ኢ', rep15)\n",
        "        rep17 = re.sub('[ዔ]', 'ኤ', rep16)\n",
        "        rep18 = re.sub('[ዕ]', 'እ', rep17)\n",
        "        rep19 = re.sub('[ዖ]', 'ኦ', rep18)\n",
        "        rep20 = re.sub('[ጸ]', 'ፀ', rep19)\n",
        "        rep21 = re.sub('[ጹ]', 'ፁ', rep20)\n",
        "        rep22 = re.sub('[ጺ]', 'ፂ', rep21)\n",
        "        rep23 = re.sub('[ጻ]', 'ፃ', rep22)\n",
        "        rep24 = re.sub('[ጼ]', 'ፄ', rep23)\n",
        "        rep25 = re.sub('[ጽ]', 'ፅ', rep24)\n",
        "        rep26 = re.sub('[ጾ]', 'ፆ', rep25)\n",
        "        rep27 = re.sub('(ሉ[ዋአ])', 'ሏ', rep26)\n",
        "        rep28 = re.sub('(ሙ[ዋአ])', 'ሟ', rep27)\n",
        "        rep29 = re.sub('(ቱ[ዋአ])', 'ቷ', rep28)\n",
        "        rep30 = re.sub('(ሩ[ዋአ])', 'ሯ', rep29)\n",
        "        rep31 = re.sub('(ሱ[ዋአ])', 'ሷ', rep30)\n",
        "        rep32 = re.sub('(ሹ[ዋአ])', 'ሿ', rep31)\n",
        "        rep33 = re.sub('(ቁ[ዋአ])', 'ቋ', rep32)\n",
        "        rep34 = re.sub('(ቡ[ዋአ])', 'ቧ', rep33)\n",
        "        rep35 = re.sub('(ቹ[ዋአ])', 'ቿ', rep34)\n",
        "        rep36 = re.sub('(ሁ[ዋአ])', 'ኋ', rep35)\n",
        "        rep37 = re.sub('(ኑ[ዋአ])', 'ኗ', rep36)\n",
        "        rep38 = re.sub('(ኙ[ዋአ])', 'ኟ', rep37)\n",
        "        rep39 = re.sub('(ኩ[ዋአ])', 'ኳ', rep38)\n",
        "        rep40 = re.sub('(ዙ[ዋአ])', 'ዟ', rep39)\n",
        "        rep41 = re.sub('(ጉ[ዋአ])', 'ጓ', rep40)\n",
        "        rep42 = re.sub('(ደ[ዋአ])', 'ዷ', rep41)\n",
        "        rep43 = re.sub('(ጡ[ዋአ])', 'ጧ', rep42)\n",
        "        rep44 = re.sub('(ጩ[ዋአ])', 'ጯ', rep43)\n",
        "        rep45 = re.sub('(ጹ[ዋአ])', 'ጿ', rep44)\n",
        "        rep46 = re.sub('(ፉ[ዋአ])', 'ፏ', rep45)\n",
        "        rep47 = re.sub('[ቊ]', 'ቁ', rep46)\n",
        "        rep48 = re.sub('[ኵ]', 'ኩ', rep47)\n",
        "\n",
        "        return rep48\n",
        "\n",
        "    def remove_punctuation_and_special_chars(self, text: str) -> str:\n",
        "        normalized_text = re.sub(r'[!@#$%^&*()…\\[\\]{};:\"›’‘\"’\\/<>?|`´~\\\\=\\+፡;]+', ' ', text)\n",
        "        return normalized_text\n",
        "\n",
        "    def remove_non_amharic_chars_and_numbers(self, text: str) -> str:\n",
        "        rm_num_and_ascii = re.sub('[A-Za-z0-9]', '', text)\n",
        "        return re.sub('[^\\\\u1200-\\\\u137F\\\\s]+', '', rm_num_and_ascii)\n",
        "\n",
        "class EnglishTextPreprocessor(TextPreprocessingPipeline):\n",
        "    def __init__(self, tokenizer: Tokenizer) -> None:\n",
        "        super().__init__(tokenizer)\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def preprocess(self, text: str, encode=True) -> str:\n",
        "        # Convert text to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Replace common English abbreviations\n",
        "        text = self.replace_english_abbreviations(text)\n",
        "\n",
        "        # Remove punctuation and special characters\n",
        "        text = self.remove_punctuation_and_special_chars(text)\n",
        "\n",
        "        # Remove non-English characters and numbers\n",
        "        text = self.remove_non_english_chars_and_numbers(text)\n",
        "\n",
        "        if encode:\n",
        "            return self.tokenizer.encode(text).ids\n",
        "        else:\n",
        "            return text\n",
        "\n",
        "    def remove_stopwords(self, words):\n",
        "        \"\"\"\n",
        "        Remove common English stopwords from the list of words.\n",
        "        \"\"\"\n",
        "        filtered_words = [word for word in words if word not in self.stop_words]\n",
        "        return filtered_words\n",
        "\n",
        "    def lemmatize(self, words):\n",
        "        \"\"\"\n",
        "        Lemmatize words to their base form.\n",
        "        \"\"\"\n",
        "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
        "        return lemmatized_words\n",
        "\n",
        "    def replace_english_abbreviations(self, text: str) -> str:\n",
        "        english_abbreviations = {\n",
        "            \"i.e.\": \"that is\",\n",
        "            \"e.g.\": \"for example\",\n",
        "            \"etc.\": \"and so on\",\n",
        "            \"mr.\": \"mister\",\n",
        "            \"mrs.\": \"missus\",\n",
        "            \"dr.\": \"doctor\",\n",
        "            \"st.\": \"saint\",\n",
        "            \"ave.\": \"avenue\",\n",
        "            \"apt.\": \"apartment\",\n",
        "            \"dept.\": \"department\",\n",
        "            \"univ.\": \"university\",\n",
        "            \"prof.\": \"professor\",\n",
        "            \"jr.\": \"junior\",\n",
        "            \"sr.\": \"senior\",\n",
        "            \"co.\": \"company\",\n",
        "            \"corp.\": \"corporation\",\n",
        "            \"inc.\": \"incorporated\",\n",
        "            \"est.\": \"established\",\n",
        "            \"jan.\": \"january\",\n",
        "            \"feb.\": \"february\",\n",
        "            \"mar.\": \"march\",\n",
        "            \"apr.\": \"april\",\n",
        "            \"jun.\": \"june\",\n",
        "            \"jul.\": \"july\",\n",
        "            \"aug.\": \"august\",\n",
        "            \"sep.\": \"september\",\n",
        "            \"oct.\": \"october\",\n",
        "            \"nov.\": \"november\",\n",
        "            \"dec.\": \"december\",\n",
        "        }\n",
        "        for key in english_abbreviations:\n",
        "            regex = rf'\\b{re.escape(key)}\\b'\n",
        "            text = re.sub(regex, english_abbreviations[key], text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_non_english_chars_and_numbers(self, text: str) -> str:\n",
        "        # Remove non-English characters\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "        # Remove numbers\n",
        "        text = re.sub(r'\\d', ' ', text)\n",
        "\n",
        "        # Remove extra spaces\n",
        "        text = re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_punctuation_and_special_chars(self, text: str) -> str:\n",
        "        normalized_text = re.sub(r'[!@#$%^&*()…\\[\\]{};:\"›’‘\"’\\/<>?|`´~\\\\=\\+፡;]+', ' ', text)\n",
        "        return normalized_text\n",
        "\n",
        "def evaluate_model(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    return avg_val_loss\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_save_path):\n",
        "    writer = SummaryWriter()\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        val_loss = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        writer.add_scalars('Loss', {'train': avg_train_loss, 'val': val_loss}, epoch)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            save_model(model, model_save_path)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "def save_model(model, path):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f'Model saved to {path}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-87XxklAwsw"
      },
      "source": [
        "DEFINE DATA PREPROCESSING PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw2YqPNFHjXX"
      },
      "outputs": [],
      "source": [
        "class PreprocessingPipeline(ABC):\n",
        "    def __init__(self, tokenizer: Tokenizer) -> None:\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize the input text into words.\n",
        "        \"\"\"\n",
        "        words = word_tokenize(text)\n",
        "        return words\n",
        "\n",
        "    @abstractmethod\n",
        "    def preprocess(self, text: str, encode=True) -> str:\n",
        "        pass\n",
        "\n",
        "\n",
        "class AmharicPreprocessor(PreprocessingPipeline):\n",
        "    def __init__(self, tokenizer: Tokenizer) -> None:\n",
        "        super().__init__(tokenizer)\n",
        "\n",
        "    def preprocess(self, text: str, encode=True) -> str:\n",
        "        # Character level mismatch\n",
        "        text = self.normalize_char_level_missmatch(text)\n",
        "\n",
        "        # Replace commonly used abbreviations\n",
        "        text = self.normalize_abbreviations(text)\n",
        "\n",
        "        # Remove punctuations and special characters\n",
        "        text = self.remove_punc_and_special_chars(text)\n",
        "\n",
        "        # Remove non-amharic chars and arabic numbers\n",
        "        text = self.remove_ascii_and_numbers(text)\n",
        "\n",
        "        if encode:\n",
        "            return self.tokenizer.encode(\n",
        "                text,\n",
        "            ).ids\n",
        "        else:\n",
        "            return text\n",
        "\n",
        "    # Remove abbreviations\n",
        "    def normalize_abbreviations(self, text: str) -> str:\n",
        "        common_amharic_abbreviations = {\n",
        "            \"ት/ቤት\": \"ትምህርት ቤት\",\n",
        "            \"ት/ርት\": \"ትምህርት\",\n",
        "            \"ት/ክፍል\": \"ትምህርት ክፍል\",\n",
        "            \"ሃ/አለቃ\": \"ሃምሳ አለቃ\",\n",
        "            \"ሃ/ስላሴ\": \"ሃይለ ስላሴ\",\n",
        "            \"ደ/ዘይት\": \"ደብረ ዘይት\",\n",
        "            \"ደ/ታቦር\": \"ደብረ ታቦር\",\n",
        "            \"መ/ር\": \"መምህር\",\n",
        "            \"መ/ቤት\": \"መስሪያ ቤት\",\n",
        "            \"መ/አለቃ\": \"መቶ አለቃ\",\n",
        "            \"ክ/ከተማ\": \"ክፍለ ከተማ\",\n",
        "            \"ክ/ሀገር\": \"ክፍለ ሀገር\",\n",
        "            \"ወ/ር\": \"\",\n",
        "            \"ወ/ሮ\": \"ወይዘሮ\",\n",
        "            \"ወ/ሪት\": \"ወይዘሪት\",\n",
        "            \"ወ/ስላሴ\": \"ወልደ ስላሴ\",\n",
        "            \"ፍ/ስላሴ\": \"ፍቅረ ስላሴ\",\n",
        "            \"ፍ/ቤት\": \"ፍርድ ቤት\",\n",
        "            \"ጽ/ቤት\": \"ጽህፈት ቤት\",\n",
        "            \"ሲ/ር\": \"\",\n",
        "            \"ፕ/ር\": \"ፕሮፌሰር\",\n",
        "            \"ጠ/ሚንስትር\": \"ጠቅላይ ሚኒስተር\",\n",
        "            \"ጠ/ሚ\": \"ጠቅላይ ሚኒስተር\",\n",
        "            \"ዶ/ር\": \"ዶክተር\",\n",
        "            \"ገ/ገዮርጊስ\": \"ገብረ ገዮርጊስ\",\n",
        "            \"ቤ/ክርስትያን\": \"ቤተ ክርስትያን\",\n",
        "            \"ም/ስራ\": \"\",\n",
        "            \"ም/ቤት\": \"ምክር ቤተ\",\n",
        "            \"ተ/ሃይማኖት\": \"ተክለ ሃይማኖት\",\n",
        "            \"ሚ/ር\": \"ሚኒስትር\",\n",
        "            \"ኮ/ል\": \"ኮሎኔል\",\n",
        "            \"ሜ/ጀነራል\": \"ሜጀር ጀነራል\",\n",
        "            \"ብ/ጀነራል\": \"ብርጋደር ጀነራል\",\n",
        "            \"ሌ/ኮለኔል\": \"ሌተናንት ኮለኔል\",\n",
        "            \"ሊ/መንበር\": \"ሊቀ መንበር\",\n",
        "            \"አ/አ\": \"ኣዲስ ኣበባ\",\n",
        "            \"አ.አ\": \"ኣዲስ ኣበባ\",\n",
        "            \"ር/መምህር\": \"ርዕሰ መምህር\",\n",
        "            \"ፕ/ት\": \"\",\n",
        "            \"ዓም\": \"ዓመተ ምህረት\",\n",
        "            \"ዓ.ዓ\": \"ዓመተ ዓለም\",\n",
        "        }\n",
        "        for key in common_amharic_abbreviations:\n",
        "            regex = rf'\\b{re.escape(key)}\\b'\n",
        "            text = re.sub(regex, common_amharic_abbreviations[key], text)\n",
        "\n",
        "        # Remove punctuation, numbers, and extra spaces\n",
        "        text = re.sub(r'[.\\?\"\\',/#!$%^&*;:፤።{}=\\-_`~()፩፪፫፬፭፮፮፰፱፲፳፴፵፵፷፸፹፺፻01-9]', ' ', text)\n",
        "        text = re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    #method to normalize character level missmatch such as ጸሀይ and ፀሐይ\n",
        "    def normalize_char_level_missmatch(self, text: str) -> str:\n",
        "        rep1=re.sub('[ሃኅኃሐሓኻ]','ሀ',text)\n",
        "        rep2=re.sub('[ሑኁዅ]','ሁ',rep1)\n",
        "        rep3=re.sub('[ኂሒኺ]','ሂ',rep2)\n",
        "        rep4=re.sub('[ኌሔዄ]','ሄ',rep3)\n",
        "        rep5=re.sub('[ሕኅ]','ህ',rep4)\n",
        "        rep6=re.sub('[ኆሖኾ]','ሆ',rep5)\n",
        "        rep7=re.sub('[ሠ]','ሰ',rep6)\n",
        "        rep8=re.sub('[ሡ]','ሱ',rep7)\n",
        "        rep9=re.sub('[ሢ]','ሲ',rep8)\n",
        "        rep10=re.sub('[ሣ]','ሳ',rep9)\n",
        "        rep11=re.sub('[ሤ]','ሴ',rep10)\n",
        "        rep12=re.sub('[ሥ]','ስ',rep11)\n",
        "        rep13=re.sub('[ሦ]','ሶ',rep12)\n",
        "        rep14=re.sub('[ዓኣዐ]','አ',rep13)\n",
        "        rep15=re.sub('[ዑ]','ኡ',rep14)\n",
        "        rep16=re.sub('[ዒ]','ኢ',rep15)\n",
        "        rep17=re.sub('[ዔ]','ኤ',rep16)\n",
        "        rep18=re.sub('[ዕ]','እ',rep17)\n",
        "        rep19=re.sub('[ዖ]','ኦ',rep18)\n",
        "        rep20=re.sub('[ጸ]','ፀ',rep19)\n",
        "        rep21=re.sub('[ጹ]','ፁ',rep20)\n",
        "        rep22=re.sub('[ጺ]','ፂ',rep21)\n",
        "        rep23=re.sub('[ጻ]','ፃ',rep22)\n",
        "        rep24=re.sub('[ጼ]','ፄ',rep23)\n",
        "        rep25=re.sub('[ጽ]','ፅ',rep24)\n",
        "        rep26=re.sub('[ጾ]','ፆ',rep25)\n",
        "        #Normalizing words with Labialized Amharic characters such as በልቱዋል or  በልቱአል to  በልቷል\n",
        "        rep27=re.sub('(ሉ[ዋአ])','ሏ',rep26)\n",
        "        rep28=re.sub('(ሙ[ዋአ])','ሟ',rep27)\n",
        "        rep29=re.sub('(ቱ[ዋአ])','ቷ',rep28)\n",
        "        rep30=re.sub('(ሩ[ዋአ])','ሯ',rep29)\n",
        "        rep31=re.sub('(ሱ[ዋአ])','ሷ',rep30)\n",
        "        rep32=re.sub('(ሹ[ዋአ])','ሿ',rep31)\n",
        "        rep33=re.sub('(ቁ[ዋአ])','ቋ',rep32)\n",
        "        rep34=re.sub('(ቡ[ዋአ])','ቧ',rep33)\n",
        "        rep35=re.sub('(ቹ[ዋአ])','ቿ',rep34)\n",
        "        rep36=re.sub('(ሁ[ዋአ])','ኋ',rep35)\n",
        "        rep37=re.sub('(ኑ[ዋአ])','ኗ',rep36)\n",
        "        rep38=re.sub('(ኙ[ዋአ])','ኟ',rep37)\n",
        "        rep39=re.sub('(ኩ[ዋአ])','ኳ',rep38)\n",
        "        rep40=re.sub('(ዙ[ዋአ])','ዟ',rep39)\n",
        "        rep41=re.sub('(ጉ[ዋአ])','ጓ',rep40)\n",
        "        rep42=re.sub('(ደ[ዋአ])','ዷ',rep41)\n",
        "        rep43=re.sub('(ጡ[ዋአ])','ጧ',rep42)\n",
        "        rep44=re.sub('(ጩ[ዋአ])','ጯ',rep43)\n",
        "        rep45=re.sub('(ጹ[ዋአ])','ጿ',rep44)\n",
        "        rep46=re.sub('(ፉ[ዋአ])','ፏ',rep45)\n",
        "        rep47=re.sub('[ቊ]','ቁ',rep46) #ቁ can be written as ቊ\n",
        "        rep48=re.sub('[ኵ]','ኩ',rep47) #ኩ can be also written as ኵ\n",
        "\n",
        "        return rep48\n",
        "\n",
        "    #replacing any existance of special character or punctuation to null\n",
        "    def remove_punc_and_special_chars(self, text: str) -> str: # puct in amh =፡።፤;፦፧፨፠፣\n",
        "        normalized_text = re.sub('[\\!\\@\\#\\$\\%\\^\\«\\»\\&\\*\\(\\)\\…\\[\\]\\{\\}\\;\\“\\”\\›\\’\\‘\\\"\\'\\:\\,\\.\\‹\\/\\<\\>\\?\\\\\\\\|\\`\\´\\~\\-\\=\\+\\፡\\።\\፤\\;\\፦\\፥\\፧\\፨\\፠\\፣]', '', text)\n",
        "        return normalized_text\n",
        "\n",
        "    #remove all ascii characters and Arabic and Amharic numbers\n",
        "    def remove_ascii_and_numbers(self, text: str) -> str:\n",
        "        rm_num_and_ascii=re.sub('[A-Za-z0-9]','',text)\n",
        "        return re.sub('[^\\u1200-\\u137F\\s]+','',rm_num_and_ascii)\n",
        "\n",
        "\n",
        "class EnglishPreprocessor(PreprocessingPipeline):\n",
        "    def __init__(self, tokenizer: Tokenizer) -> None:\n",
        "        super().__init__(tokenizer)\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def preprocess(self, text: str, encode=True) -> str:\n",
        "\n",
        "        # Lowercase the text\n",
        "        text = text.lower()\n",
        "\n",
        "        # Replace commonly used English abbreviations\n",
        "        text = self.normalize_english_abbreviations(text)\n",
        "\n",
        "        # Remove punctuations and special characters\n",
        "        text = self.remove_punc_and_special_chars(text)\n",
        "\n",
        "        # Remove non-English chars and numbers\n",
        "        text = self.remove_non_english_and_numbers(text)\n",
        "\n",
        "        # # Pre-tokenization\n",
        "        # words = self.tokenize(text)\n",
        "\n",
        "        # # Remove stopwords\n",
        "        # words = self.remove_stopwords(words)\n",
        "\n",
        "        # # Lemmatization\n",
        "        # words = self.lemmatize(words)\n",
        "\n",
        "        if encode:\n",
        "            return self.tokenizer.encode(\n",
        "                text\n",
        "            ).ids\n",
        "        else:\n",
        "            return text\n",
        "\n",
        "    def remove_stopwords(self, words):\n",
        "        \"\"\"\n",
        "        Remove common English stopwords from the list of words.\n",
        "        \"\"\"\n",
        "        filtered_words = [word for word in words if word not in self.stop_words]\n",
        "        return filtered_words\n",
        "\n",
        "    def lemmatize(self, words):\n",
        "        \"\"\"\n",
        "        Lemmatize words to their base form.\n",
        "        \"\"\"\n",
        "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
        "        return lemmatized_words\n",
        "\n",
        "    def normalize_english_abbreviations(self, text: str) -> str:\n",
        "        common_english_abbreviations = {\n",
        "            \"i.e.\": \"that is\",\n",
        "            \"e.g.\": \"for example\",\n",
        "            \"etc.\": \"and so on\",\n",
        "            \"mr.\": \"mister\",\n",
        "            \"mrs.\": \"missus\",\n",
        "            \"dr.\": \"doctor\",\n",
        "            \"st.\": \"saint\",\n",
        "            \"ave.\": \"avenue\",\n",
        "            \"apt.\": \"apartment\",\n",
        "            \"dept.\": \"department\",\n",
        "            \"univ.\": \"university\",\n",
        "            \"prof.\": \"professor\",\n",
        "            \"jr.\": \"junior\",\n",
        "            \"sr.\": \"senior\",\n",
        "            \"co.\": \"company\",\n",
        "            \"corp.\": \"corporation\",\n",
        "            \"inc.\": \"incorporated\",\n",
        "            \"est.\": \"established\",\n",
        "            \"jan.\": \"january\",\n",
        "            \"feb.\": \"february\",\n",
        "            \"mar.\": \"march\",\n",
        "            \"apr.\": \"april\",\n",
        "            \"jun.\": \"june\",\n",
        "            \"jul.\": \"july\",\n",
        "            \"aug.\": \"august\",\n",
        "            \"sep.\": \"september\",\n",
        "            \"oct.\": \"october\",\n",
        "            \"nov.\": \"november\",\n",
        "            \"dec.\": \"december\",\n",
        "            # Add more abbreviations as needed\n",
        "        }\n",
        "        for key in common_english_abbreviations:\n",
        "            regex = rf'\\b{re.escape(key)}\\b'\n",
        "            text = re.sub(regex, common_english_abbreviations[key], text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_non_english_and_numbers(self, text: str) -> str:\n",
        "        # Remove non-English characters\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "        # Remove numbers\n",
        "        text = re.sub(r'\\d', ' ', text)\n",
        "\n",
        "        # Remove extra spaces\n",
        "        text = re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    #replacing any existance of special character or punctuation to null\n",
        "    def remove_punc_and_special_chars(self, text: str) -> str:\n",
        "        normalized_text = re.sub('[\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\…\\[\\]\\{\\}\\;\\“\\”\\›\\’\\‘\\\"\\'\\:\\,\\.\\‹\\/\\<\\>\\?\\\\\\\\|\\`\\´\\~\\-\\=\\+\\፡\\;]', ' ', text)\n",
        "        return normalized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG8QDAnNH_P2"
      },
      "source": [
        "DATASET PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOvqb527A3NZ"
      },
      "outputs": [],
      "source": [
        "class ParallelTextDataset(Dataset):\n",
        "    def __init__(self, dataset: list[dict], src_tokenizer: Tokenizer, tgt_tokenizer: Tokenizer) -> None:\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "        self.src_preprocessor = EnglishPreprocessor(src_tokenizer)\n",
        "        self.tgt_preprocessor = AmharicPreprocessor(tgt_tokenizer)\n",
        "\n",
        "        self.sos_token = torch.tensor([self.src_tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64)  # (1,)\n",
        "        self.eos_token = torch.tensor([self.src_tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64)  # (1,)\n",
        "        self.pad_token = torch.tensor([self.src_tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)  # (1,)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def batch_iterator(self, batch_size: int) -> DataLoader:\n",
        "        return DataLoader(self, batch_size, shuffle=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def lookback_mask(size: int) -> torch.Tensor:\n",
        "        # Lower triangular matrix\n",
        "        # [[\n",
        "        #   [1, 0, ... , 0],\n",
        "        #   [1, 1, ... , 0],\n",
        "        #   [1, 1, ... , 0],\n",
        "        #   [1, 1, ... , 1]\n",
        "        # ]]\n",
        "        # 1 x size x size\n",
        "        return torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int) == 0\n",
        "\n",
        "    def __getitem__(self, index) -> dict:\n",
        "        src_tgt_pair = self.dataset[index]\n",
        "        src_text = src_tgt_pair[SRC_LANG]\n",
        "        tgt_text = src_tgt_pair[TGT_LANG]\n",
        "\n",
        "        src_token_ids = self.src_preprocessor.preprocess(src_text)\n",
        "        tgt_token_ids = self.tgt_preprocessor.preprocess(tgt_text)\n",
        "\n",
        "        src_padding = SEQ_LEN - len(src_token_ids) - 2\n",
        "        tgt_padding = SEQ_LEN - len(tgt_token_ids) - 1\n",
        "\n",
        "        # (seq_len,)\n",
        "        encoder_input = torch.concat([\n",
        "            self.sos_token,                                                     # (1,)\n",
        "            torch.tensor(src_token_ids, dtype=torch.int64),                     # (len(src_token_ids),)\n",
        "            self.eos_token,                                                     # (1,)\n",
        "            torch.tensor([self.pad_token] * src_padding, dtype=torch.int64)     # (src_padding,)\n",
        "        ])\n",
        "\n",
        "        # (seq_len,)\n",
        "        decoder_input = torch.concat([\n",
        "            self.sos_token,                                                     # (1,)\n",
        "            torch.tensor(tgt_token_ids, dtype=torch.int64),                     # (len(tgt_token_ids),)\n",
        "            torch.tensor([self.pad_token] * tgt_padding, dtype=torch.int64)     # (tgt_padding,)\n",
        "        ])\n",
        "\n",
        "        # (seq_len,)\n",
        "        label = torch.concat([\n",
        "            torch.tensor(tgt_token_ids, dtype=torch.int64),                     # (len(tgt_token_ids),)\n",
        "            self.eos_token,                                                     # (1,)\n",
        "            torch.tensor([self.pad_token] * tgt_padding, dtype=torch.int64)     # (tgt_padding,)\n",
        "        ])\n",
        "\n",
        "        return {\n",
        "            # (seq_len,)\n",
        "            \"encoder_input\": encoder_input,\n",
        "\n",
        "            # (seq_len,)\n",
        "            \"decoder_input\": decoder_input,\n",
        "\n",
        "            # (seq_len,) != (1,) --> (seq_len,) --> (1, 1, seq_len)\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "\n",
        "            # (seq_len,) != (1,) --> (seq_len,) --> (1, 1, seq_len) --> (1, seq_len) & (1, seq_len, seq_len) --> (1, seq_len, seq_len)\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & self.lookback_mask(SEQ_LEN),\n",
        "\n",
        "            # (seq_len,)\n",
        "            \"label\": label,\n",
        "\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drT29PM0BFpG"
      },
      "source": [
        "DEFINE TRANSFORMER MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yOmUJB-BCTA"
      },
      "outputs": [],
      "source": [
        "class WordEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding: nn.Embedding = nn.Embedding(vocab_size, D_MODEL)\n",
        "\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): (batches, seq_len, 1)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: (batches, seq_len, d_model)\n",
        "    \"\"\"\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.embedding.forward(x) * math.sqrt(D_MODEL)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # To randomly zero-out a given tensor based on the given probability to combat overfitting\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "        '''\n",
        "            Create the positional encoding using the following formula:\n",
        "                PE(pos, 2i) = sin(pos / (10000 ^ (2i/d_model)))\n",
        "                PE(pos, 2i + 1) = cos(pos / (10000 ^ (2i/d_model)))\n",
        "        '''\n",
        "        # Create a matrix of shape (max_seq_len, d_model)\n",
        "        pe = torch.zeros(SEQ_LEN, D_MODEL)\n",
        "\n",
        "        # Create a vector of shape (max_seq_len, 1)\n",
        "        pos = torch.arange(0, SEQ_LEN, dtype=torch.float).float().unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, D_MODEL, 2).float() * -(math.log(10000.0) / D_MODEL))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0) # (1, max_seq_len, d_model)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): (batches, seq_len, d_model) where  0 < seq_len < self.max_seq_len\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: (batches, seq_len, d_model)\n",
        "    \"\"\"\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert x.shape[1] <= SEQ_LEN, f\"Input sequence length exceeds the position encoder's max sequence length  `{SEQ_LEN}`\"\n",
        "        return self.dropout(x + self.pe[:, :x.shape[1], :].requires_grad_(False))\n",
        "\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(D_MODEL, DFF).to(DEVICE)\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "        self.linear_2 = nn.Linear(DFF, D_MODEL).to(DEVICE)\n",
        "\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): (batches, seq_len, d_model) where  0 < seq_len < self.max_seq_len\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: (batches, seq_len, d_model)\n",
        "    \"\"\"\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # (batches, seq_len, d_model) -> (batches, seq_len, dff) -> (batches, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        assert D_MODEL % HEADS == 0, \"d_model is not divisible by heads\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_k = D_MODEL // HEADS\n",
        "\n",
        "        self.W_q = nn.Linear(D_MODEL, D_MODEL, bias=False).to(DEVICE)\n",
        "        self.W_k = nn.Linear(D_MODEL, D_MODEL, bias=False).to(DEVICE)\n",
        "        self.W_v = nn.Linear(D_MODEL, D_MODEL, bias=False).to(DEVICE)\n",
        "\n",
        "        self.W_o = nn.Linear(D_MODEL, D_MODEL, bias=False).to(DEVICE)\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            query (torch.Tensor): (batches, heads, seq_len, d_k) where  0 < seq_len < self.max_seq_len\n",
        "            key (torch.Tensor): (batches, heads, seq_len, d_k) where  0 < seq_len < self.max_seq_len\n",
        "            value (torch.Tensor): (batches, heads, seq_len, d_k) where  0 < seq_len < self.max_seq_len\n",
        "\n",
        "            dropout (nn.Dropout): -\n",
        "            mask (torch.Tensor): -\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: (batches, heads, seq_len, d_k)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, dropout: nn.Dropout=None, mask: torch.Tensor=None) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        d_k = query.shape[-1]\n",
        "\n",
        "        # (batches, heads, seq_len, d_k) @ (batches, heads, d_k, seq_len) --> (batches, heads, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "\n",
        "        # Here we apply the lookback mask so that the output at a certain position(which is a token)\n",
        "        # can only depend on the tokens on the previous positions. We also apply the ignore masks\n",
        "        # so that attention score for the padding special token [PAD] is zero.\n",
        "        if mask is not None:\n",
        "            attention_scores.masked_fill_(mask == 0, -1e09)\n",
        "\n",
        "        # (batches, heads, seq_len, seq_len) which applies softmax to the last dimension\n",
        "        # so that the sum of the probabilities along this dimension equals 1\n",
        "        attention_scores = attention_scores.softmax(dim=-1)\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "\n",
        "        # (batches, heads, seq_len, seq_len) @ (batches, heads, seq_len, d_k) --> (batches, heads, seq_len, d_k)\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    # q must be of shape (batches, seq_len, self.d_model) where  0 < seq_len < self.max_seq_len\n",
        "    # k must be of shape (batches, seq_len, self.d_model) where  0 < seq_len < self.max_seq_len\n",
        "    # v must be of shape (batches, seq_len, self.d_model) where  0 < seq_len < self.max_seq_len\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            query (torch.Tensor): (batches, seq_len, d_model) where  0 < seq_len < self.max_seq_len\n",
        "            key (torch.Tensor): (batches, seq_len, d_model) where  0 < seq_len < self.max_seq_len\n",
        "            value (torch.Tensor): (batches, seq_len, d_model) where  0 < seq_len < self.max_seq_len\n",
        "\n",
        "            mask (torch.Tensor): -\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: (batches, seq_len, d_model)\n",
        "    \"\"\"\n",
        "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
        "        query: torch.Tensor = self.W_q(q) # (batches, seq_len, d_model) @ (d_model, d_model) --> (batches, seq_len, d_model)\n",
        "        key: torch.Tensor = self.W_k(k)   # (batches, seq_len, d_model) @ (d_model, d_model) --> (batches, seq_len, d_model)\n",
        "        value: torch.Tensor = self.W_v(v) # (batches, seq_len, d_model) @ (d_model, d_model) --> (batches, seq_len, d_model)\n",
        "\n",
        "        # (batches, seq_len, d_model) --> (batches, seq_len, heads, d_k) --> (batches, heads, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], HEADS, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], HEADS, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], HEADS, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Here has shape x = (batches, heads, seq_len, d_k)\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, self.dropout, mask)\n",
        "\n",
        "        # (batches, heads, seq_len, d_k) --> (batches, seq_len, heads, d_k)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # (batches, seq_len, heads, d_k) --> (batches, seq_len, d_model)\n",
        "        x = x.contiguous().view(x.shape[0], -1, HEADS * self.d_k)\n",
        "\n",
        "        # (batches, seq_len, d_model) --> (batches, seq_len, d_model)\n",
        "        return self.W_o(x)\n",
        "\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "        self.norm = nn.LayerNorm(D_MODEL, device=DEVICE)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, sublayer: nn.Module) -> torch.Tensor:\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection() for _ in range(2)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        return self.residual_connections[1](x, self.feed_forward_block)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoder_blocks: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder_blocks = encoder_blocks\n",
        "        self.norm = nn.LayerNorm(D_MODEL, device=DEVICE)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
        "        for block in self.encoder_blocks:\n",
        "            x = block(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection() for _ in range(3)])\n",
        "\n",
        "    # Since this transformer model is for translation we have a src_mask(from the encoder) and tgt_mask(from the decoder) which are two different languages\n",
        "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, decoder_blocks: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.decoder_blocks = decoder_blocks\n",
        "        self.norm = nn.LayerNorm(D_MODEL, device=DEVICE)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
        "        for layer in self.decoder_blocks:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(D_MODEL, vocab_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # (batches, seq_len, d_model) --> (batches, seq_len, vocab_size)\n",
        "        return self.proj(x)\n",
        "\n",
        "class MtTransformerModel(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: WordEmbedding, tgt_embed: WordEmbedding, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x: torch.Tensor):\n",
        "        return self.projection_layer(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def build(\n",
        "        src_vocab_size: int,\n",
        "        tgt_vocab_size: int\n",
        "    ):\n",
        "        # Create the embedding layers\n",
        "        src_embed = WordEmbedding(src_vocab_size)\n",
        "        tgt_embed = WordEmbedding(tgt_vocab_size)\n",
        "\n",
        "        # Create the positional encoding layers\n",
        "        src_pos = PositionalEncoding()\n",
        "        tgt_pos = PositionalEncoding()\n",
        "\n",
        "        # Create N_BLOCKS number of encoders\n",
        "        encoder_blocks = []\n",
        "        for _ in range(N_BLOCKS):\n",
        "            self_attention_block = MultiHeadAttentionBlock()\n",
        "            feed_forward_block = FeedForwardBlock()\n",
        "\n",
        "            encoder_blocks.append(\n",
        "                EncoderBlock(self_attention_block, feed_forward_block)\n",
        "            )\n",
        "\n",
        "        # Create N_BLOCKS number of decoders\n",
        "        decoder_blocks = []\n",
        "        for _ in range(N_BLOCKS):\n",
        "            self_attention_block = MultiHeadAttentionBlock()\n",
        "            cross_attention_block = MultiHeadAttentionBlock()\n",
        "            feed_forward_block = FeedForwardBlock()\n",
        "\n",
        "            decoder_blocks.append(\n",
        "                DecoderBlock(self_attention_block, cross_attention_block, feed_forward_block)\n",
        "            )\n",
        "\n",
        "        # Create the encoder and the decoder\n",
        "        encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "        decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "        # Create the projection layer\n",
        "        projection_layer = ProjectionLayer(tgt_vocab_size)\n",
        "\n",
        "        # Create the transformer\n",
        "        transformer = MtTransformerModel(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "        # Initialize the parameters\n",
        "        for p in transformer.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "        return transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z90--4DbXW8"
      },
      "source": [
        "DEFINE THE INFERENCE ENGINE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpKzI9RsbXoy"
      },
      "outputs": [],
      "source": [
        "class MtInferenceEngine:\n",
        "\n",
        "    def __init__(self, model: MtTransformerModel, src_tokenizer: Tokenizer, tgt_tokenizer: Tokenizer, top_k: int= 5, nucleus_threshold=10) -> None:\n",
        "        self.model = model\n",
        "        self.top_k = top_k\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.nucleus_threshold = nucleus_threshold\n",
        "        self.sos_token = torch.tensor([self.src_tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64, device=DEVICE)  # (1,)\n",
        "        self.eos_token = torch.tensor([self.src_tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64, device=DEVICE)  # (1,)\n",
        "        self.pad_token = torch.tensor([self.src_tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64, device=DEVICE)  # (1,)\n",
        "        self.model.eval()\n",
        "\n",
        "    def translate(self, source_text: str, max_len: int) -> tuple[str, str]:\n",
        "        dataset = ParallelTextDataset(\n",
        "            dataset=[{\"en\": source_text, \"am\":\"\" }],\n",
        "            src_tokenizer=self.src_tokenizer,\n",
        "            tgt_tokenizer=self.tgt_tokenizer\n",
        "        )\n",
        "        batch_iterator = iter( dataset.batch_iterator(1))\n",
        "        batch = next(batch_iterator)\n",
        "\n",
        "        encoder_input = batch[\"encoder_input\"].to(DEVICE)       # (1, seq_len)\n",
        "        encoder_mask = batch[\"encoder_mask\"].to(DEVICE)         # (1, 1, 1, seq_len)\n",
        "        decoder_mask = batch[\"decoder_mask\"].to(DEVICE)         # (1, 1, seq_len, seq_len)\n",
        "\n",
        "        return self.translate_raw(encoder_input, encoder_mask, decoder_mask, max_len)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def translate_raw(self, encoder_input: torch.Tensor, encoder_mask: torch.Tensor, decoder_mask: torch.Tensor, max_len: int) -> str:\n",
        "        sos_idx = self.tgt_tokenizer.token_to_id('[SOS]')\n",
        "        eos_idx = self.tgt_tokenizer.token_to_id('[EOS]')\n",
        "\n",
        "        # Precompute the encoder output and reuse it for every step\n",
        "        encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "\n",
        "        # Initialize the decoder input with the sos token\n",
        "        decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(encoder_input).to(DEVICE)\n",
        "        while decoder_input.size(1) < max_len and next_token != eos_idx:\n",
        "            # Build required masking for decoder input\n",
        "            decoder_mask = ParallelTextDataset.lookback_mask(decoder_input.size(1)).type_as(encoder_mask).to(DEVICE)\n",
        "\n",
        "            # Calculate output of decoder\n",
        "            decoder_out = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)       # (1, seq_len, d_model)\n",
        "\n",
        "            # Retrieve the embedded vector form of the last token\n",
        "            last_token_vec = decoder_out[:, -1]                         # (1, d_model)\n",
        "\n",
        "            # Get the model's raw output(logits)\n",
        "            last_token_logits = model.project(last_token_vec)           # (1, d_model) --> (1, tgt_vocab_size)\n",
        "\n",
        "            # Evaluate the probability distribution across the vocab_size\n",
        "            # dimension using softmax\n",
        "            last_token_prob = torch.softmax(last_token_logits, dim=1)\n",
        "\n",
        "            # Greedily pick the one with the highest probability\n",
        "            _, next_token = torch.max(last_token_prob, dim=1)\n",
        "\n",
        "            # Append to the decoder input for the subsequent iterations\n",
        "            decoder_input = torch.cat([\n",
        "                decoder_input,\n",
        "                torch.empty(1, 1).type_as(encoder_input).fill_(next_token.item()).to(DEVICE)\n",
        "            ], dim=1)\n",
        "\n",
        "        # Remove the batch dimension\n",
        "        decoder_input = decoder_input.squeeze(0)                                    # torch.tensor([...]) with shape tensor.Size([max_len])\n",
        "        return self.tgt_tokenizer.decode(decoder_input.detach().cpu().tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ocqdxZqBSGx"
      },
      "source": [
        "START TRAINING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_odB9LLCF27"
      },
      "outputs": [],
      "source": [
        "TOKENIZER_FOLDER = f\"{PROJECT_DIR}/tokenizers\"\n",
        "TOKENIZER_BASENAME = \"tokenizer-{0}-v3.5-12k.json\"\n",
        "import os\n",
        "def get_tokenizer(lang: str, basename: str = TOKENIZER_BASENAME) -> Tokenizer:\n",
        "    tokenizer_filename = f\"{basename.format(lang)}\"\n",
        "    tokenizer_path = os.path.join(TOKENIZER_FOLDER, tokenizer_filename)  # Use os.path.join\n",
        "\n",
        "    # Check if the tokenizer file exists\n",
        "    if not os.path.exists(tokenizer_path):\n",
        "        raise FileNotFoundError(f\"Tokenizer file not found: {tokenizer_path}\")\n",
        "\n",
        "    tokenizer: Tokenizer = Tokenizer.from_file(tokenizer_path)\n",
        "\n",
        "    tokenizer.enable_truncation(max_length=SEQ_LEN - 2)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def get_dataset() -> tuple[ParallelTextDataset, ParallelTextDataset, ParallelTextDataset]:\n",
        "    import random # Import random here if not already imported globally\n",
        "\n",
        "    with open(DATASET_PATH, 'r', encoding='utf-8') as data:\n",
        "        dataset = json.load(data)\n",
        "\n",
        "    cropped_size = int(0.10 * len(dataset))\n",
        "    cropped_dataset = random.sample(dataset, cropped_size)  # Crop the dataset here\n",
        "\n",
        "    src_tokenizer = get_tokenizer(SRC_LANG)\n",
        "    tgt_tokenizer = get_tokenizer(TGT_LANG)\n",
        "\n",
        "    train_dataset = ParallelTextDataset(cropped_dataset, src_tokenizer, tgt_tokenizer)\n",
        "    val_dataset = ParallelTextDataset(cropped_dataset, src_tokenizer, tgt_tokenizer)\n",
        "    test_dataset = ParallelTextDataset(cropped_dataset, src_tokenizer, tgt_tokenizer)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "def get_model(src_vocab_size: int, tgt_vocab_size):\n",
        "    return MtTransformerModel.build(\n",
        "        src_vocab_size=src_vocab_size,\n",
        "        tgt_vocab_size=tgt_vocab_size\n",
        "    )\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model: MtTransformerModel, val_batch_iterator: DataLoader, loss_func: nn.CrossEntropyLoss):\n",
        "    \"\"\"\n",
        "        Set the transformer module(the model) to evaluation mode\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    val_losses = []\n",
        "    # Evaluate model with `num_examples` number of random examples\n",
        "    for batch in val_batch_iterator:\n",
        "        # Retrieve the data points from the current batch\n",
        "        encoder_input = batch[\"encoder_input\"].to(DEVICE)       # (batches, seq_len)\n",
        "        decoder_input = batch[\"decoder_input\"].to(DEVICE)       # (batches, seq_len)\n",
        "        encoder_mask = batch[\"encoder_mask\"].to(DEVICE)         # (bathes, 1, 1, seq_len)\n",
        "        decoder_mask = batch[\"decoder_mask\"].to(DEVICE)         # (bathes, 1, seq_len, seq_len)\n",
        "        label: torch.Tensor = batch['label'].to(DEVICE)         # (batches, seq_len)\n",
        "\n",
        "        # Perform the forward pass according to the operations defined in\n",
        "        # the transformer model in order to build the computation graph of the model\n",
        "        encoder_output = model.encode(encoder_input, encoder_mask)                                  # (batches, seq_len, d_model)\n",
        "        decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)    # (batches, seq_len, d_model)\n",
        "        proj_output: torch.Tensor = model.project(decoder_output)                                   # (batches, seq_len, tgt_vocab_size)\n",
        "\n",
        "        # Compute the cross entropy loss\n",
        "        loss: torch.Tensor = loss_func.forward(\n",
        "            proj_output.view(-1, val_dataset.tgt_tokenizer.get_vocab_size()),     # (batches, seq_len, tgt_vocab_size) --> (batches*seq_len, tgt_vocab_size)\n",
        "            label.view(-1)                                                          # (batches, seq_len) --> (batches * seq_len, )\n",
        "        )\n",
        "\n",
        "        val_losses.append(loss.item())\n",
        "\n",
        "        if len(val_losses) > 1:\n",
        "            break\n",
        "\n",
        "    return sum(val_losses) / len(val_losses)\n",
        "\n",
        "\n",
        "def train(model: MtTransformerModel, train_dataset: ParallelTextDataset, val_dataset: ParallelTextDataset) -> None:\n",
        "    # Configure Tensorboard\n",
        "    writer = SummaryWriter(TB_LOG_DIR)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=INIT_LR, eps=1e-09)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    if PRELOAD_MODEL_SUFFIX:\n",
        "        model_filename = get_weights_file_path(PRELOAD_MODEL_SUFFIX)\n",
        "        print(f\"Preloading model {model_filename}\")\n",
        "\n",
        "        state = torch.load(model_filename)\n",
        "        initial_epoch = state[\"epoch\"] + 1\n",
        "        global_step = state[\"global_step\"]\n",
        "\n",
        "        model.load_state_dict(state[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
        "\n",
        "    loss_func = nn.CrossEntropyLoss(ignore_index=train_dataset.src_tokenizer.token_to_id('[PAD]'), label_smoothing=0.1).to(DEVICE)\n",
        "\n",
        "    batch_iterator = train_dataset.batch_iterator(BATCH_SIZE)\n",
        "    val_batch_iterator = val_dataset.batch_iterator(BATCH_SIZE)\n",
        "\n",
        "    prev_loss = float('inf')\n",
        "    val_loss = 0\n",
        "    for epoch in range(initial_epoch, EPOCHS):\n",
        "        # Wrap train_dataloader with tqdm to show a progress bar to show\n",
        "        # how much of the batches have been processed on the current epoch\n",
        "        batch_iterator = tqdm(batch_iterator, desc=f\"Processing epoch {epoch: 02d}\", colour=\"BLUE\")\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        # Iterate through the batches\n",
        "        for batch in batch_iterator:\n",
        "            \"\"\"\n",
        "                Set the transformer module(the model) to back to training mode\n",
        "            \"\"\"\n",
        "            model.train()\n",
        "\n",
        "            # Retrieve the data points from the current batch\n",
        "            encoder_input = batch[\"encoder_input\"].to(DEVICE)       # (batches, seq_len)\n",
        "            decoder_input = batch[\"decoder_input\"].to(DEVICE)       # (batches, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(DEVICE)         # (bathes, 1, 1, seq_len)\n",
        "            decoder_mask = batch[\"decoder_mask\"].to(DEVICE)         # (bathes, 1, seq_len, seq_len)\n",
        "            label: torch.Tensor = batch['label'].to(DEVICE)         # (batches, seq_len)\n",
        "\n",
        "            # Perform the forward pass according to the operations defined in\n",
        "            # the transformer model in order to build the computation graph of the model\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)                                  # (batches, seq_len, d_model)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)    # (batches, seq_len, d_model)\n",
        "            proj_output: torch.Tensor = model.project(decoder_output)                                   # (batches, seq_len, tgt_vocab_size)\n",
        "\n",
        "            # Compute the training loss\n",
        "            train_loss: torch.Tensor = loss_func.forward(\n",
        "                proj_output.view(-1, train_dataset.tgt_tokenizer.get_vocab_size()),     # (batches, seq_len, tgt_vocab_size) --> (batches*seq_len, tgt_vocab_size)\n",
        "                label.view(-1)                                                          # (batches, seq_len) --> (batches * seq_len, )\n",
        "            )\n",
        "\n",
        "            if global_step % 200 == 0:\n",
        "                # Evaluate the model on the validation dataset(aka unseen data)\n",
        "                val_loss = validate(model, val_batch_iterator, loss_func)\n",
        "\n",
        "                # Log the training and validation loss on tensorboard\n",
        "                writer.add_scalars(\"Cross-Entropy-Loss\", { \"Training\": train_loss.item(), \"Validation\": val_loss }, global_step)\n",
        "            else:\n",
        "                writer.add_scalars(\"Cross-Entropy-Loss\", { \"Training\": train_loss.item() }, global_step)\n",
        "\n",
        "            writer.flush()\n",
        "\n",
        "            # Add the calculated training loss and validation loss as a postfix to the progress bar shown by tqdm\n",
        "            batch_iterator.set_postfix({\"train_loss\": f\"{train_loss.item():6.3f}\"})\n",
        "\n",
        "            # Perform the backward pass on the computation graph built during the forward pass,\n",
        "            # in order to calculate the grad for each of the intermediate and leaf tensors on the computation graph\n",
        "            train_loss.backward()\n",
        "\n",
        "            # Update the model parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Zero the gradients of the model parameters to prevent gradient accumulation\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            train_losses.append(train_loss.item())\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        current_avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "        current_avg_val_loss = sum(val_losses) / len(val_losses)\n",
        "\n",
        "        if current_avg_train_loss < prev_loss:\n",
        "            prev_loss = current_avg_train_loss\n",
        "\n",
        "            # Save the model at the end of every epoch\n",
        "            model_filename = get_weights_file_path(f\"epoch-{epoch:02d}_avgTrainLoss-{current_avg_train_loss:6.3f}_avgValLoss-{current_avg_val_loss:6.3f}_batch-{BATCH_SIZE}_init_lr-{INIT_LR:.0e}\")\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"global_step\": global_step\n",
        "            }, model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2ptqaUz7ASq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def get_weights_file_path(suffix: str):\n",
        "    \"\"\"Generates the file path for saving model weights.\"\"\"\n",
        "    return os.path.join(MODEL_DIR, f\"{MODEL_PREFIX}{suffix}.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvV5W5qMqOww",
        "outputId": "3d81486e-0fae-4274-929e-50a2001aa0d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch  0:   0%|\u001b[34m          \u001b[0m| 1/509 [00:41<5:54:45, 41.90s/it, train_loss=9.927]"
          ]
        }
      ],
      "source": [
        "print(f\"Training started on `{DEVICE}` device\")\n",
        "DATASET_PATH = f\"{PROJECT_DIR}/data/parallel-corpus-en-am-v3.5.json\"\n",
        "SRC_LANG = \"en\"\n",
        "TGT_LANG = \"am\"\n",
        "SEQ_LEN = 52\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = get_dataset()\n",
        "\n",
        "D_MODEL = 512\n",
        "N_BLOCKS = 6\n",
        "HEADS = 8  # Reduced from 32 to 8 for better performance\n",
        "DROPOUT = 0.1\n",
        "DFF = 2048\n",
        "TB_LOG_DIR = f\"{PROJECT_DIR}/logs/custom_model\"\n",
        "INIT_LR = 1e-4\n",
        "PRELOAD_MODEL_SUFFIX = \"\"\n",
        "\n",
        "\n",
        "model = get_model(train_dataset.src_tokenizer.get_vocab_size(), train_dataset.tgt_tokenizer.get_vocab_size()).to(DEVICE)\n",
        "\n",
        "train(model, train_dataset, val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0dm_bm4qTe_"
      },
      "outputs": [],
      "source": [
        "print(f\"Testing started on `{DEVICE}` device\")\n",
        "model.eval()\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss(ignore_index=train_dataset.src_tokenizer.token_to_id('[PAD]'), label_smoothing=0.1).to(DEVICE)\n",
        "\n",
        "batch_iterator = tqdm(val_dataset.batch_iterator(BATCH_SIZE), desc=f\"Evaluating model on test dataset\", colour=\"GREEN\")\n",
        "losses = []\n",
        "# Iterate through the batches\n",
        "for batch in batch_iterator:\n",
        "    # Retrieve the data points from the current batch\n",
        "    encoder_input = batch[\"encoder_input\"].to(DEVICE)       # (batches, seq_len)\n",
        "    decoder_input = batch[\"decoder_input\"].to(DEVICE)       # (batches, seq_len)\n",
        "    encoder_mask = batch[\"encoder_mask\"].to(DEVICE)         # (bathes, 1, 1, seq_len)\n",
        "    decoder_mask = batch[\"decoder_mask\"].to(DEVICE)         # (bathes, 1, seq_len, seq_len)\n",
        "    label: torch.Tensor = batch['label'].to(DEVICE)         # (batches, seq_len)\n",
        "\n",
        "    # Perform the forward pass according to the operations defined in\n",
        "    # the transformer model in order to build the computation graph of the model\n",
        "    encoder_output = model.encode(encoder_input, encoder_mask)                                  # (batches, seq_len, d_model)\n",
        "    decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)    # (batches, seq_len, d_model)\n",
        "    proj_output: torch.Tensor = model.project(decoder_output)                                   # (batches, seq_len, tgt_vocab_size)\n",
        "\n",
        "    # Compute the training loss\n",
        "    test_loss: torch.Tensor = loss_func(\n",
        "        proj_output.view(-1, train_dataset.tgt_tokenizer.get_vocab_size()),     # (batches, seq_len, tgt_vocab_size)  -->  (batches*seq_len, tgt_vocab_size)\n",
        "        label.view(-1)                                                          # (batches, seq_len)                  -->  (batches * seq_len, )\n",
        "    )\n",
        "\n",
        "    # Add the calculated test loss as a postfix to the progress bar shown by tqdm\n",
        "    batch_iterator.set_postfix({\"test_loss\": f\"{test_loss.item():6.3f}\"})\n",
        "\n",
        "    losses.append(test_loss.item())\n",
        "\n",
        "avg_loss = sum(losses) / len(losses)\n",
        "print(f\"\\nTesting finished with an average cross entropy of {avg_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhVJ2h3DqVZA"
      },
      "outputs": [],
      "source": [
        "inference_engine = MtInferenceEngine(model, train_dataset.src_tokenizer, train_dataset.tgt_tokenizer)\n",
        "user_input = input(\"Enter a short english sentence: \")\n",
        "prediction = inference_engine.translate(user_input, 10)\n",
        "print(f\"\\n Predicted: {prediction}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3TIXQvqdwLH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the file path\n",
        "model_filename = \"/content/drive/My Drive/Ashu_NLP/models/tmodel_epoch-100.pt\"\n",
        "\n",
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), model_filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
